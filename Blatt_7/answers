2) Bei einer Matrixgröße von 2000x2000 wurde die Matrixmultiplikation mit unterschiedlichen Threadzahlen durchgeführt. Beim Compilieren wurde das Flag "-O3" verwendet. Das Optimum wurde bei 40 Threads gefunden mit: XXXXX. Die Ausführungszeit des seriellen Programmms wurde über 100 Durchläufe auf dem Host gemittelt. Dies ergab 4,382s. Der Speedup liegt damit bei

3) Um die Zeit des Datentransfers zu messen, wurde vor Beginn des "pragma omp target data"-Blocks und unmittelbar danach mit der getTime()-Methode die Zeit gemessen und voneinander subtrahiert. Das gleiche passiert vor und nach dem Ende des BLocks. Die Zeit bezieht sich dabei jeweils auf den Host (was wichtig ist, da die Uhren von Host und Phi unterschiedlich sind!). Außerdem ist es wichtig, dass die Matrixmutliplikation pro Programmausführung nur einmal durchgeführt wird, da der Compiler sonst die unnötigen Kopiervorgänge herausoptimiert. Bei achtmaliger Ausführung ergeben sich die folgenden Zeiten für die Kopiervorgänge:

4) Der Speedup auf dem Accelerator gegenüber dem seriellen Programm auf dem Host ist relativ gering. Dies kann auf zweii Faktoren zurückgeführt werden. Einerse verursacht das Erstellen der Threads einen gewissen Overhead. Andererseits kann die Host-CPU durch ihre höhere Taktung den Vorteil durch die Parallelisierung bis zu einem gewissen Grad ausgleichen. Wird das serielle Programm beispielsweise auf dem Phi direkt ausegührt (kompiliert mit -mmic) benötigt dies ca. 12 Sekunden. In diesem Fall liegt der Speedup bei 4, was schon besser ist.
Die Übertragungszeiten der Daten sind stark unterschiedlich. Das liegt daran, dass mehr Daten hin, als zurückkopoert werden müssen. Außerdem ist beim hinkopieren eine Allokation nötig, die möglicherweise mehr Zeit in Anspruch nimmt.
Der Code wurde von Anfang an möglichst optimal gehalten. In Aufgabe 2 wurde 40 Threads als optimale Anzahl ermittelt. Eventuell ist es effektiver, spaltenweise zu parallelisieren um weniger Cache-Misses zu erzeugen, je nach dem, wie die Daten im Speicher abgelegt sind.
