2) Das serielle Programm wurde mit "icc -o serial.o mmSerial.c -vec- -std=c99", RUN_COUNT=10, MAT_SIZE = 2048 kompiliert und ausgeführt. Die Option -vec- wird verwendet, damit der Compiler keine automatische vektorisierung vornimmt, um den Speedup vergleichbarer zu machen. Wie experimentell ermittelt werden konnte, hat der Einsatz von OpenMP eine Asuwirkung darauf, wie effizient der Code vektorisiert werden kann. Dabei ergab sich eine durchschnittliche Ausführungszeit (nur die Funktion matMult()) von 5,525 Sekunden.
Das parallele Programm wurde mit "icc -o parallel.o mmParallel.c -qopenmp -vec- -std=c99", RUN_COUNT=5, MAT_SIZE = 2048, MIN_THREAD_COUNT=10, MAX_THREAD_COUNT=240, STEP_SIZE=5 kompiliert und ausgeführt. Dabei ergab sich die schnellste durchschnittliche Ausführungszeit bei 230-Threads mit 0.984 Sekunden.
Der Speedup beträgt also ca. 5,6.

3) Um die Zeit des Datentransfers zu messen, wurde ein zusätzlicher "pragma omp target data"-Block eingefügt. Unmittelbar dvor und nach Beginn des Blocks wurde mit der getTime()-Methode die Zeit gemessen und voneinander subtrahiert. Das gleiche passiert vor und nach dem Ende des BLocks. Die Zeit bezieht sich dabei jeweils auf den Host (was wichtig ist, da die Uhren von Host und Phi unterschiedlich sind!). Außerdem ist es wichtig, dass die Matrixmutliplikation pro Programmausführung nur einmal durchgeführt wird, da der Compiler sonst die unnötigen Kopiervorgänge herausoptimiert. Bei achtmaliger Ausführung ergeben sich die folgenden Zeiten für die Kopiervorgänge:

4) Der Speedup zur seriellen Version ist nicht wie eventuell angenommen werden könnte 60 oder 120. Die hat mehrere Gründe. Die Erstellung von 230 Threads erzeugt zusätzlichen einen Overhead, der für nur eine einzige Matrixmultiplikation stark ins Gewicht fällt. Außerdem sind die Kerne auf dem Xeon Phi langsamer getaktet. Die Transferzeit zur Karte hin ist größer als von der Karte weg. Das liegt daran, dass drei Matrizen hinwärts kopiert werden müssen und nur eine zurück. Dennoch ist das Verhältnis größer als 1:3. Dies könnte daran liegen, dass beim Kopiervorgang hinwärts auch eine Allokation des Speicherplatzes stattfinden muss und außerdem ein Prozess auf dem Accelerator gestartet werden muss. Die Übertragungszeit wird zwar mitgemessen, fällt jedoch kaum ins gewicht, da der Compiler die unnötigen Kopiervorgänge bei mehreren Ausführungen hintereinander herausoptimiert.
Die optimale Threadanzahl wurde bereits in Aufgabe 2 ermittelt und beträgt 230. Das Programm kann weiter optimiert werden, in dem die SIMD-Einheiten zusätzlich programmiert werden. Die eigene Inplementierung erreicht einen Wert von durchschnittlich 474ms.
